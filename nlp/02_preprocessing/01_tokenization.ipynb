{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**토큰화 목적**\n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'NLP is fascinating. It has many applications in real-world scenarios'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios']\n",
      "['NLP is fascinating.', 'It has many applications in real-world scenarios']\n",
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios']\n",
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 단어 토큰화\n",
    "print(nltk.word_tokenize(text))\n",
    "\n",
    "# 문장 토큰화\n",
    "print(nltk.sent_tokenize(text))\n",
    "\n",
    "# 문장별 단어 토큰화\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함 => 어휘 크기를 줄이고 다양한 언어 패턴 학습 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un', '##ha', '##pp', '##iness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 분리 과정\n",
    "1. WordPiece 토크나이저는 사전에 등록된 단어 우선 사용\n",
    "    - Bert의 WordPiece 토크나이저는 최대한 긴 단어를 먼저 찾으려는 방식으로 동작\n",
    "    - 'happy'가 사전에 있으면 그대로 사용, 없으면 가장 긴 서브워드 조합 찾아 나눔\n",
    "2. 'happy'가 사전에 포함되지 않은 경우\n",
    "    - 일반적으로 'happy'는 BERT의 사전에 포함되어 있지만, 학습된 모델에 없을 수 있음\n",
    "    - 'happy'가 사전에 없는 경우 사전에 있는 조각(서브워드)로 분해\n",
    "3. 그래서 과정은..\n",
    "    1. 'unhappiness' 검색 -> 없음\n",
    "    2. 'un' 검색 -> 있음 -> 유지\n",
    "    3. 'happiness' 검색 -> 없음\n",
    "    4. 'ha' 검색 -> 있음\n",
    "    5. 'pp' 검색 -> 있음\n",
    "    6. 'iness' 검색 -> 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 주의사항\n",
    "1. 구두점이나 특수 문자를 단순 제외하면 안됨\n",
    "2. 줄임말, 단어 내 띄어쓰기 유의\n",
    "\n",
    "**표준 토큰화 예제 *Penn Treebank Tokenization**\n",
    "- 규칙 1: 하이픈으로 구성된 단어는 하나로 유지한다.\n",
    "- 규칙 2: doesn't와 같이 '가 있는 단어는 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'files',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'fruit',\n",
       " 'files',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점을 제외한 단어 토큰화\n",
    "import re\n",
    "\n",
    "text = 'Time files like an arrow; fruit files like a banana.'\n",
    "re.findall(r'\\b\\w+\\b', text)\n",
    "# r'' : 파이썬 raw text (이스케이핑 문자를 문자 그대로 처리)\n",
    "# \\b : 경계 문자 (boundary / 공백, 구두점, ...)\n",
    "# \\w : 글자 (word / 영문자, 숫자, _)\n",
    "# \\w+ : 수령자 (하나 이상)\n",
    "# => 경게 문자로 감싸진 하나 이상의 글자를 전부 찾아라~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer\n",
    "# 단어/구두점으로 토큰을 구분 (', - 포함 단어도 분리)\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "word_punct_tokenizer.tokenize(text)\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/02/18', '날짜', '표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다. \\\n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/02/18 날짜 표현에 사용될 수 있다. \\\n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "\n",
    "print(treebank_word_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kss==5.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['배경은 1920년대의 경성부이다.', '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.', \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문.\", '멍청이, 꼰대...가 아닐수 없다.', '사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"배경은 1920년대의 경성부이다. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문. 멍청이, 꼰대...가 아닐수 없다. 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('files', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = 'Time files like an arrow'\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.cli.download('en_core_web_sm')      # 직접 다운로드\n",
    "spacy_nlp = spacy.load('en_core_web_sm')    # 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "files : NOUN\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n"
     ]
    }
   ],
   "source": [
    "tokens= spacy_nlp(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from konlpy) (1.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from konlpy) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\sky_env\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '버스', '를', '기다리고', '있다', '.', '지각', '은', '면', '하겠군', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = '나는 버스를 기다리고 있다. 지각은 면하겠군!'\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "morphs = okt.morphs(text)\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('버스', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('기다리고', 'Verb'),\n",
       " ('있다', 'Adjective'),\n",
       " ('.', 'Punctuation'),\n",
       " ('지각', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('면', 'Noun'),\n",
       " ('하겠군', 'Verb'),\n",
       " ('!', 'Punctuation')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석\n",
    "pos_tag = okt.pos(text)\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '버스', '지각', '면']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = okt.nouns(text)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
