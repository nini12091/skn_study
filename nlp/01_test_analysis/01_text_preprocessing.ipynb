{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리 NLP | 텍스트 분석 Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install nltk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.__version__\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = 'NLTK is a powerful library for NLP!!!'\n",
    "word_tokenize(text)     # 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Matrix is everywhere its all around us, here even in this room.\n",
    "You can see it out your window or on your television.\n",
    "You feel it when you go to work, or go to church or pay your taxes.'''\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장별 단어 토큰화\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [word_tokenize(sentence) for sentence in sentences ]\n",
    "\n",
    "result = tokenize_text(text)\n",
    "print(len(result), len(result[0]), len(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram\n",
    "from nltk import ngrams\n",
    "\n",
    "text = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "bigram = ngrams(tokens, 2)\n",
    "print([token for token in bigram])\n",
    "\n",
    "trigram = ngrams(tokens, 3)\n",
    "print([token for token in trigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어(stopwords) 제거\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids()\n",
    "\n",
    "print(len(stopwords.words('english')))\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "tokens = []\n",
    "for word in word_tokenize(text):    # 토큰화\n",
    "    word = word.lower()     # 소문자 변환\n",
    "    if word not in stopwords_list:  # 불용어 처리\n",
    "        tokens.append(word)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW -CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text1 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "text2 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe\\\n",
    "You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "\n",
    "texts = [text1, text2]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(texts)\n",
    "text_vecs = count_vectorizer.transform(texts)\n",
    "\n",
    "print(text_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_vectorizer.vocabulary_)\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 등장 횟수 구하기\n",
    "word_counts = text_vecs.toarray().sum(axis=0)\n",
    "\n",
    "# 단어 데이터프레임에 빈도 추가\n",
    "vocab_df['count'] = vocab_df['idx'].apply(lambda i: word_counts[i])\n",
    "\n",
    "# idx 열 제거 (불필요함)\n",
    "vocab_df = vocab_df.drop(columns=['idx'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 52)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(text_vecs.toarray().shape)\n",
    "\n",
    "vocab = sorted(count_vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "vocab_df = pd.DataFrame(vocab, columns=['word', 'idx'])\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 52)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'bed believe', 'believe', 'believe want', 'believeyou',\n",
       "       'believeyou red', 'blue', 'blue pill', 'church', 'church pay',\n",
       "       'deep', 'deep rabbit', 'ends', 'ends wake', 'feel', 'feel work',\n",
       "       'goes', 'hole', 'hole goes', 'matrix', 'matrix room', 'pay',\n",
       "       'pay taxes', 'pill', 'pill stay', 'pill story', 'rabbit',\n",
       "       'rabbit hole', 'red', 'red pill', 'room', 'room window', 'stay',\n",
       "       'stay wonderland', 'story', 'story ends', 'taxes', 'television',\n",
       "       'television feel', 'wake', 'wake bed', 'want', 'want believeyou',\n",
       "       'window', 'window television', 'wonderland', 'wonderland deep',\n",
       "       'work', 'work church'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),      # n-gram 범위 지정 (최소값, 최대값) / 기본값 (1, 1)\n",
    "    max_features=30          # 빈도 수 상위인 n개의 데이터 사용\n",
    ")   \n",
    "\n",
    "texts_vecs = count_vectorizer.fit_transform(texts)\n",
    "print(text_vecs.toarray().shape)\n",
    "\n",
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW -TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF == Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "        0.        , 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.33333333, 0.33333333, 0.        , 0.        , 0.        ,\n",
       "        0.33333333, 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "        0.        , 0.        , 0.33333333, 0.        , 0.33333333],\n",
       "       [0.22941573, 0.22941573, 0.22941573, 0.22941573, 0.        ,\n",
       "        0.22941573, 0.22941573, 0.        , 0.22941573, 0.22941573,\n",
       "        0.        , 0.        , 0.45883147, 0.22941573, 0.22941573,\n",
       "        0.        , 0.22941573, 0.22941573, 0.        , 0.        ,\n",
       "        0.22941573, 0.22941573, 0.        , 0.22941573, 0.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['bed', 'believe', 'believeyou', 'blue', 'church', 'deep', 'ends',\n",
       "       'feel', 'goes', 'hole', 'matrix', 'pay', 'pill', 'rabbit', 'red',\n",
       "       'room', 'stay', 'story', 'taxes', 'television', 'wake', 'want',\n",
       "       'window', 'wonderland', 'work'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "texts_vecs = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "display(texts_vecs.toarray())\n",
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
